#!/usr/bin/env python

import re
import httplib
import urllib
from datetime import datetime,date,timedelta
import logging
import time


import lxml.html
import feedparser
import requests


from churn import util, fuzzydate
from churn.basescraper import BaseScraper



class PRNewsWireScraper(BaseScraper):
    """ scraper to grab press releases from prnewswire """

    name = 'prnewswire'
    doc_type = 1

    def __init__(self):
        super(PRNewsWireScraper, self).__init__()
        self.parser.add_option('-r', '--range', action="store", dest="range", help="""grab press releases in date range (eg "2011-01-01..2011-01-05" or "2011-01-01" for a single day)""")



    def go(self,options):
        if options.range is None:
            super(PRNewsWireScraper, self).go(options)

        else:
            # handle a historical range of days
            day_pat = re.compile(r"^\s*(\d{4}-\d{2}-\d{2})\s*$")
            m = day_pat.match(options.range)
            if m:
                day_from = datetime.strptime(m.group(1),'%Y-%m-%d').date()
                day_to = day_from
            else:
                rng_pat = re.compile("^\s*(\d{4}-\d{2}-\d{2})[.][.](\d{4}-\d{2}-\d{2})\s*$")
                m = rng_pat.match(options.range)
                day_from = datetime.strptime(m.group(1),'%Y-%m-%d').date()
                day_to = datetime.strptime(m.group(2),'%Y-%m-%d').date()

            assert(day_from<=day_to)

            day = day_from
            while True:
                logging.info("Processing %s", day)
                urls = self.find_historical(day.year, day.month, day.day)
                self.process_batch(urls)
                day += timedelta(days=1)
                if day > day_to:
                    break


    def find_latest(self):
        """ find latest set of press releases"""
        feed_url = "http://www.prnewswire.com/rss/all-news-releases-from-PR-newswire-news.rss"

        logging.debug("read feed %s", feed_url)
        feed = feedparser.parse(feed_url)
        all_urls = [e.link for e in feed.entries]
        return all_urls


    def find_historical(self,year,month,day):
        """ return press release URLs from prnewswire for a specific day """

        # prnewswire lets you search backward from a particular date
        # no rss though - so we have to scrape the html, page by page until we
        # have a full days worth

        found = []
        page = 1    # increase as we go back in time
        done = False
        while not done:
            params = {
                'year': year,
                'month': month-1,   # months are 0=indexed
                'day': day,
                'hour': 23,     # includes 23:00-00:00
                'page': page
                }
        
            list_url = "http://www.prnewswire.com/news-releases/english-releases/?" + urllib.urlencode(params)
            response = requests.get(list_url)
            if response.status_code == httplib.OK:

                doc = lxml.html.fromstring(response.content)
                doc.make_links_absolute(base_url=list_url)


                for li in doc.cssselect("div.block02 ul"):
                    a = li.cssselect("a")[0]
                    try:
                        (dt,span) = fuzzydate.parse_date(li.cssselect(".seo-h3-datetime")[0].text_content())
                    except:
                        #a link with something besides a url (like language dropdown)
                        continue
                    # hit a new day?
                    if dt.year != year or dt.month != month or dt.day != day:
                        done = True
                        break

                    href = a.get('href')
                    logging.debug('Will scrape: ' + href)
                    found.append(href)

            page += 1
            if page > 200:
                break
        return found


    # output fields:
    # url
    # date  - unix timestamp
    # title
    # source
    # text
    # topics    - comma-separated topics
    # location
    # language - 'en_us', 'fr', 'es' etc...
    def extract(self,html, url):
        """ extract prnewswire press release data from a page of html """

        # eg
        # "EAST PEORIA, Ill., Oct. 24, 2011 /PRNewswire/ -- "
        # "NUEVA YORK, 26 de octubre de 2011 /PRNewswire-HISPANIC PR WIRE/ --"
        dateline_pat = re.compile(r'^\s*(?P<location>.*?),\s*(?P<month>\w{3,})[.]?\s*(?P<day>\d{1,2}),\s*(?P<year>\d{4})\s*/(?P<wire>.*?)/\s*--\s*',re.MULTILINE)

        # pattern to extract prnewswire internal id from url
        # eg http://www.prnewswire.com/news-releases/dnainfocom-announces-nycs-first-uptown-beauty-month-through-november-133436033.html
        id_pat = re.compile(r'-(\d{5,}).html')

        # eg "SOURCE blahcorp inc."
        source_pat = re.compile(r'^(?:SOURCE|FUENTE)\s+(?P<source>.*?)\s*$',re.MULTILINE)

        # cruft elements to strip before extracting text (if you add to this, DON'T FORGET THE COMMAS! :-)
        cruft_sel = 'table, script, style, .newsreldettrans, .horizontalline, .clearboth, #dvWideRelease, #linktopagetop'

        out = {'url':url}

        id = int(id_pat.search(url).group(1))
        out['id'] = id

        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(base_url=url)

        topics = set()
        foo = doc.cssselect('.col-1 .seo-h4-seemorereleases')
        if foo:
            topics_div = foo[0]
            for a in topics_div.findall('a'):
                topic = unicode(a.get('title')).strip()
                topic = re.sub(',','',topic)    # kill commas
                topics.add(topic)
        out['topics'] = u','.join(topics)

        # "<meta http-equiv='Content-Language' content='es'/>"
        lang_meta = doc.cssselect('meta[http-equiv="Content-Language"]')[0]
        out['language'] = unicode(lang_meta.get('content')).lower()

        # use title to identify div containing main content text
        head = doc.cssselect('#dvHead')[0]
        main = head.getparent()
        out['title'] = unicode(head.text_content()).strip()

        # grab text
        [cruft.drop_tree() for cruft in main.cssselect(cruft_sel)]
        content = util.render_text(main)
        content = re.compile(r'[\t ]{1,}',re.DOTALL).sub(' ',content)
        content = re.compile(r'\s{2,}$',re.MULTILINE).sub('\n',content)
        out['text'] = content

        # eg "SOURCE blahcorp inc."
        m = source_pat.search(content)
        if m:
            out['source'] = m.group('source')
        else:
            out['source'] = u''

        # break up dateline
        m = dateline_pat.search(content)
        if m:
            year = int(m.group('year'))
            month = util.lookup_month(m.group('month'))
            day = int(m.group('day'))
            pubdate = date(year,month,day)
            location = m.group('location')
        else:
            # TODO: handle non-english dates
            pubdate = datetime.now()
            location = u''

        out['date'] = int(time.mktime(pubdate.timetuple())*1000)
        out['location'] = location

        return out



if __name__ == '__main__':
    scraper = PRNewsWireScraper()
    scraper.main()



